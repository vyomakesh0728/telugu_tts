{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPmljTBAN7z/pIxNry7AM5u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "205dfae89c674c4fa1196dec3d641b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fbbe4ee41cd0403599624e5cf995a41a",
              "IPY_MODEL_97e41dcc3c6e430b98116cd3b546a388",
              "IPY_MODEL_c6679f582b484034b3bdb8e042f020f5"
            ],
            "layout": "IPY_MODEL_43bb1b535a8e4ba981d98e33590952cd"
          }
        },
        "fbbe4ee41cd0403599624e5cf995a41a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b5f5eab64b43198125ab56850e9ca7",
            "placeholder": "​",
            "style": "IPY_MODEL_76275a2860114785839c2a93a9e8c567",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "97e41dcc3c6e430b98116cd3b546a388": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b74f3e220413402e9bc70aff3e53efb7",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_07c960615cfb4daa9db12364317f91bf",
            "value": 10
          }
        },
        "c6679f582b484034b3bdb8e042f020f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ebc648cf79a4a26a27e4497726d21f7",
            "placeholder": "​",
            "style": "IPY_MODEL_78bcdfdb8b3241748da5bd90945be968",
            "value": " 10/10 [00:53&lt;00:00,  4.97s/it]"
          }
        },
        "43bb1b535a8e4ba981d98e33590952cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44b5f5eab64b43198125ab56850e9ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "76275a2860114785839c2a93a9e8c567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b74f3e220413402e9bc70aff3e53efb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "07c960615cfb4daa9db12364317f91bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ebc648cf79a4a26a27e4497726d21f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78bcdfdb8b3241748da5bd90945be968": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "964d3c761d09402aa3a526c47dcaf6dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_110fe134f23648888a3190f1539a0c71",
              "IPY_MODEL_7a5a6d67bacf4df2a30e40c26133eafc",
              "IPY_MODEL_988b75eecd144e6bad866f4fb027f2c7"
            ],
            "layout": "IPY_MODEL_493ae5c357af47cfa10e665e9a01e8c1"
          }
        },
        "110fe134f23648888a3190f1539a0c71": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d365f2b1f816431581b60b7b01573acb",
            "placeholder": "​",
            "style": "IPY_MODEL_cde3b123b6214487938f85fda5dbaff6",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "7a5a6d67bacf4df2a30e40c26133eafc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bfaaa007c37c4e968a0c9e4e7b78a83e",
            "max": 456,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5225816cf29948cab34c7a6cef861074",
            "value": 456
          }
        },
        "988b75eecd144e6bad866f4fb027f2c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c2852651fb4747da8f217cc519a0d222",
            "placeholder": "​",
            "style": "IPY_MODEL_d51a8983d6c84175bdd29cdb283502e6",
            "value": " 456/456 [00:00&lt;00:00, 709.57 examples/s]"
          }
        },
        "493ae5c357af47cfa10e665e9a01e8c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d365f2b1f816431581b60b7b01573acb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cde3b123b6214487938f85fda5dbaff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfaaa007c37c4e968a0c9e4e7b78a83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5225816cf29948cab34c7a6cef861074": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2852651fb4747da8f217cc519a0d222": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d51a8983d6c84175bdd29cdb283502e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad08872a225848f4ae86ccd499b42271": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2fcae60810a41428927ba9eeace3515",
              "IPY_MODEL_692154cac4b34abba51a72e16cf402b7",
              "IPY_MODEL_5fd451a0b6d646fea51fbd4027ffbdd3"
            ],
            "layout": "IPY_MODEL_907befd4d7c64dad80b159a2347c174f"
          }
        },
        "e2fcae60810a41428927ba9eeace3515": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71339c1808bd4995a6ef928500710eef",
            "placeholder": "​",
            "style": "IPY_MODEL_227985559dab46b9a3fe7b1340b27571",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "692154cac4b34abba51a72e16cf402b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_838ae8395a2a4e128917ba7c837ae6ea",
            "max": 410,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d45d00b441104f0a8b196adee8bf52d9",
            "value": 410
          }
        },
        "5fd451a0b6d646fea51fbd4027ffbdd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf5cdb577a264c43a314982a47b755fa",
            "placeholder": "​",
            "style": "IPY_MODEL_be7957ee0896409e8dba57fe1cec3a4f",
            "value": " 410/410 [00:00&lt;00:00, 945.17 examples/s]"
          }
        },
        "907befd4d7c64dad80b159a2347c174f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71339c1808bd4995a6ef928500710eef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227985559dab46b9a3fe7b1340b27571": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "838ae8395a2a4e128917ba7c837ae6ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d45d00b441104f0a8b196adee8bf52d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bf5cdb577a264c43a314982a47b755fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be7957ee0896409e8dba57fe1cec3a4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1b57a4f4e96a4ffbaa83632da8b0962e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f212ef89abc42b08fac55badab3219b",
              "IPY_MODEL_b67c4ecb2ed24205ad73a4c4c65be95a",
              "IPY_MODEL_2e6a580fb81648e581e31c20bc7784a2"
            ],
            "layout": "IPY_MODEL_e609fcdd965d4b5cbafc6bb8d02472c9"
          }
        },
        "7f212ef89abc42b08fac55badab3219b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bb5af8a864f4aacbc3b8f4bb5573071",
            "placeholder": "​",
            "style": "IPY_MODEL_a2252e42e281482ab956f94d10d27094",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "b67c4ecb2ed24205ad73a4c4c65be95a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4e7d309905543d2b8a726f78b52b279",
            "max": 46,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b2ac83f4e66a46a194f2605f6f8afbc7",
            "value": 46
          }
        },
        "2e6a580fb81648e581e31c20bc7784a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9ae157376db4480ae479f202ce314a7",
            "placeholder": "​",
            "style": "IPY_MODEL_de64311d03ce4eb1bb7df643d776bbe9",
            "value": " 46/46 [00:00&lt;00:00, 96.25 examples/s]"
          }
        },
        "e609fcdd965d4b5cbafc6bb8d02472c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0bb5af8a864f4aacbc3b8f4bb5573071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2252e42e281482ab956f94d10d27094": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4e7d309905543d2b8a726f78b52b279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b2ac83f4e66a46a194f2605f6f8afbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f9ae157376db4480ae479f202ce314a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de64311d03ce4eb1bb7df643d776bbe9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vyomakesh0728/telugu_tts/blob/main/sarvam_got_emotions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ip5AOLMMm6R8",
        "outputId": "5ee1e0b8-2f15-4746-c635-101e5cbcce19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.2/40.2 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.5/10.5 MB\u001b[0m \u001b[31m101.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m362.1/362.1 kB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-polars-cu12 25.2.2 requires polars<1.22,>=1.20, but you have polars 1.30.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!pip install -q --upgrade \\\n",
        "    \"transformers[torch]>=4.40\" peft bitsandbytes accelerate \\\n",
        "    datasets polars ninja\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers, inspect, sys, os\n",
        "print(\"Transformers version →\", transformers.__version__)\n",
        "print(\"Loaded from          →\", os.path.abspath(transformers.__file__))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gEgJtrzOxTA",
        "outputId": "d1458f22-b1f6-405d-f94b-d82b31aee598"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version → 4.52.3\n",
            "Loaded from          → /usr/local/lib/python3.11/dist-packages/transformers/__init__.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import inspect, transformers\n",
        "print(\"Transformers =\", transformers.__version__)\n",
        "print(\"evaluation_strategy\" in inspect.signature(transformers.TrainingArguments.__init__).parameters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM2de9ZTPCk8",
        "outputId": "4ab71175-1247-4cd2-dbd3-99f6401e855b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers = 4.52.3\n",
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, random, time\n",
        "from datasets import load_dataset\n",
        "from transformers import (AutoTokenizer, AutoModelForCausalLM,\n",
        "                          BitsAndBytesConfig, TrainingArguments, Trainer,\n",
        "                          TrainerCallback, set_seed)\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "import gc\n",
        "\n",
        "\n",
        "import torch\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.set_device(0)\n",
        "\n",
        "\n",
        "def try_load_model(model_id, bnb_cfg, tokenizer, emotion_tokens, lora_cfg, max_retries=3):\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            print(f\"\\n[Attempt {attempt+1}] Loading model...\")\n",
        "            # --- CLEAR VRAM/RAM ---\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            time.sleep(2)\n",
        "\n",
        "            # --- Load tokenizer + resize for extra tokens ---\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "            if tokenizer.pad_token_id is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "            if tokenizer.add_tokens(emotion_tokens) > 0:\n",
        "                print(\"Added emotion tokens to vocab\")\n",
        "\n",
        "\n",
        "            # --- Try loading model ---\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_id,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                quantization_config=bnb_cfg,\n",
        "                device_map=\"auto\",  # auto-partition across GPU/CPU\n",
        "            )\n",
        "\n",
        "            model = prepare_model_for_kbit_training(\n",
        "                model,\n",
        "                use_gradient_checkpointing=True     # keeps VRAM low\n",
        "            )\n",
        "\n",
        "            model.resize_token_embeddings(len(tokenizer))\n",
        "            model.gradient_checkpointing_enable()\n",
        "            print(\"Model loaded: modules are split across GPU/CPU as needed.\")\n",
        "\n",
        "            # --- Attach LoRA adapters ---\n",
        "            model = get_peft_model(model, lora_cfg)\n",
        "            n_requires_grad = sum(p.requires_grad for p in model.parameters())\n",
        "            n_total = sum(1 for _ in model.parameters())\n",
        "            print(f\"{n_requires_grad} / {n_total} parameters require grad\")\n",
        "            assert n_requires_grad > 0, \"No parameters require grad! Something is frozen or LoRA not set up.\"\n",
        "            print(\"LoRA adapters attached.\")\n",
        "\n",
        "            for n, p in model.named_parameters():\n",
        "              if p.requires_grad:\n",
        "                print(\"Trainable:\", n, p.shape)\n",
        "            assert any(p.requires_grad for p in model.parameters()), \"No parameters require grad!\"\n",
        "\n",
        "\n",
        "            return model, tokenizer\n",
        "\n",
        "        except RuntimeError as e:\n",
        "            if \"out of memory\" in str(e) or \"CUDA out of memory\" in str(e):\n",
        "                print(\"[WARN] CUDA OOM! Attempting to clear memory and offload more to CPU...\")\n",
        "                torch.cuda.empty_cache()\n",
        "                gc.collect()\n",
        "                time.sleep(2)\n",
        "                continue  # retry up to max_retries\n",
        "            else:\n",
        "                raise e  # any other runtime error should be surfaced\n",
        "    raise RuntimeError(\"Failed to load model after multiple attempts due to insufficient GPU memory.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bXf8oWZv-GWa"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- CONFIGURATION ---\n",
        "model_id = \"sarvamai/sarvam-m\"\n",
        "emotion_tokens = [\"<angry>\", \"<happy>\", \"<neutral>\", \"<sad>\", \"<surprised>\"]\n",
        "\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,  # <== CRUCIAL for hybrid CPU/GPU\n",
        ")\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=16, lora_alpha=32, lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    bias=\"none\",\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "\n",
        "\n",
        "# --- SMART MODEL LOADING ---\n",
        "model, tokenizer = try_load_model(model_id, bnb_cfg, tokenizer, emotion_tokens, lora_cfg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "205dfae89c674c4fa1196dec3d641b41",
            "fbbe4ee41cd0403599624e5cf995a41a",
            "97e41dcc3c6e430b98116cd3b546a388",
            "c6679f582b484034b3bdb8e042f020f5",
            "43bb1b535a8e4ba981d98e33590952cd",
            "44b5f5eab64b43198125ab56850e9ca7",
            "76275a2860114785839c2a93a9e8c567",
            "b74f3e220413402e9bc70aff3e53efb7",
            "07c960615cfb4daa9db12364317f91bf",
            "6ebc648cf79a4a26a27e4497726d21f7",
            "78bcdfdb8b3241748da5bd90945be968"
          ]
        },
        "id": "O4y0uGV6oUBq",
        "outputId": "c4a008ce-971b-4ae2-eb3a-15100f75c3a4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Attempt 1] Loading model...\n",
            "Added emotion tokens to vocab\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "205dfae89c674c4fa1196dec3d641b41"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded: modules are split across GPU/CPU as needed.\n",
            "320 / 683 parameters require grad\n",
            "LoRA adapters attached.\n",
            "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
            "Trainable: base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight torch.Size([4096, 16])\n",
            "Trainable: base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
            "Trainable: base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight torch.Size([1024, 16])\n",
            "Trainable: base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight torch.Size([16, 4096])\n",
            "Trainable: base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokenizer type:\", type(tokenizer))\n",
        "print(\"Vocab size:\", tokenizer.vocab_size)\n",
        "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
        "print(\"All added tokens:\", tokenizer.additional_special_tokens)\n",
        "\n",
        "for token in emotion_tokens:\n",
        "    print(f\"Token '{token}':\", tokenizer.convert_tokens_to_ids(token))\n",
        "\n",
        "\n",
        "test_text = \"Emotion: <happy>\\nText: This is a test.\"\n",
        "tok_out = tokenizer(test_text)\n",
        "print(\"Input IDs:\", tok_out[\"input_ids\"])\n",
        "print(\"Decoded:\", tokenizer.decode(tok_out[\"input_ids\"]))\n",
        "\n",
        "\n",
        "print(\"Tokenizer files:\", tokenizer.pretrained_vocab_files_map)\n",
        "print(\"Tokenizer name or path:\", tokenizer.name_or_path)\n",
        "\n",
        "tokenizer.padding_side = \"right\"\n",
        "if tokenizer.pad_token is None:\n",
        "    # Check if <pad> is in vocab, otherwise use eos_token\n",
        "    if \"<pad>\" in tokenizer.get_vocab():\n",
        "        tokenizer.pad_token = \"<pad>\"\n",
        "    else:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "tok_out = tokenizer(\"Hello world!\", max_length=16, padding=\"max_length\", truncation=True)\n",
        "print(tok_out)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k_Nms_sAa6o",
        "outputId": "305818d7-2c1f-4e8e-f4ab-77a615ceb45e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenizer type: <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>\n",
            "Vocab size: 131072\n",
            "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>', 'additional_special_tokens': ['<unk>', '<s>', '</s>', '[INST]', '[/INST]', '[AVAILABLE_TOOLS]', '[/AVAILABLE_TOOLS]', '[TOOL_RESULTS]', '[/TOOL_RESULTS]', '[TOOL_CALLS]', '[IMG]', '<pad>', '[IMG_BREAK]', '[IMG_END]', '[PREFIX]', '[MIDDLE]', '[SUFFIX]', '[SYSTEM_PROMPT]', '[/SYSTEM_PROMPT]', '[TOOL_CONTENT]', '<SPECIAL_20>', '<SPECIAL_21>', '<SPECIAL_22>', '<SPECIAL_23>', '<SPECIAL_24>', '<SPECIAL_25>', '<SPECIAL_26>', '<SPECIAL_27>', '<SPECIAL_28>', '<SPECIAL_29>', '<SPECIAL_30>', '<SPECIAL_31>', '<SPECIAL_32>', '<SPECIAL_33>', '<SPECIAL_34>', '<SPECIAL_35>', '<SPECIAL_36>', '<SPECIAL_37>', '<SPECIAL_38>', '<SPECIAL_39>', '<SPECIAL_40>', '<SPECIAL_41>', '<SPECIAL_42>', '<SPECIAL_43>', '<SPECIAL_44>', '<SPECIAL_45>', '<SPECIAL_46>', '<SPECIAL_47>', '<SPECIAL_48>', '<SPECIAL_49>', '<SPECIAL_50>', '<SPECIAL_51>', '<SPECIAL_52>', '<SPECIAL_53>', '<SPECIAL_54>', '<SPECIAL_55>', '<SPECIAL_56>', '<SPECIAL_57>', '<SPECIAL_58>', '<SPECIAL_59>', '<SPECIAL_60>', '<SPECIAL_61>', '<SPECIAL_62>', '<SPECIAL_63>', '<SPECIAL_64>', '<SPECIAL_65>', '<SPECIAL_66>', '<SPECIAL_67>', '<SPECIAL_68>', '<SPECIAL_69>', '<SPECIAL_70>', '<SPECIAL_71>', '<SPECIAL_72>', '<SPECIAL_73>', '<SPECIAL_74>', '<SPECIAL_75>', '<SPECIAL_76>', '<SPECIAL_77>', '<SPECIAL_78>', '<SPECIAL_79>', '<SPECIAL_80>', '<SPECIAL_81>', '<SPECIAL_82>', '<SPECIAL_83>', '<SPECIAL_84>', '<SPECIAL_85>', '<SPECIAL_86>', '<SPECIAL_87>', '<SPECIAL_88>', '<SPECIAL_89>', '<SPECIAL_90>', '<SPECIAL_91>', '<SPECIAL_92>', '<SPECIAL_93>', '<SPECIAL_94>', '<SPECIAL_95>', '<SPECIAL_96>', '<SPECIAL_97>', '<SPECIAL_98>', '<SPECIAL_99>', '<SPECIAL_100>', '<SPECIAL_101>', '<SPECIAL_102>', '<SPECIAL_103>', '<SPECIAL_104>', '<SPECIAL_105>', '<SPECIAL_106>', '<SPECIAL_107>', '<SPECIAL_108>', '<SPECIAL_109>', '<SPECIAL_110>', '<SPECIAL_111>', '<SPECIAL_112>', '<SPECIAL_113>', '<SPECIAL_114>', '<SPECIAL_115>', '<SPECIAL_116>', '<SPECIAL_117>', '<SPECIAL_118>', '<SPECIAL_119>', '<SPECIAL_120>', '<SPECIAL_121>', '<SPECIAL_122>', '<SPECIAL_123>', '<SPECIAL_124>', '<SPECIAL_125>', '<SPECIAL_126>', '<SPECIAL_127>', '<SPECIAL_128>', '<SPECIAL_129>', '<SPECIAL_130>', '<SPECIAL_131>', '<SPECIAL_132>', '<SPECIAL_133>', '<SPECIAL_134>', '<SPECIAL_135>', '<SPECIAL_136>', '<SPECIAL_137>', '<SPECIAL_138>', '<SPECIAL_139>', '<SPECIAL_140>', '<SPECIAL_141>', '<SPECIAL_142>', '<SPECIAL_143>', '<SPECIAL_144>', '<SPECIAL_145>', '<SPECIAL_146>', '<SPECIAL_147>', '<SPECIAL_148>', '<SPECIAL_149>', '<SPECIAL_150>', '<SPECIAL_151>', '<SPECIAL_152>', '<SPECIAL_153>', '<SPECIAL_154>', '<SPECIAL_155>', '<SPECIAL_156>', '<SPECIAL_157>', '<SPECIAL_158>', '<SPECIAL_159>', '<SPECIAL_160>', '<SPECIAL_161>', '<SPECIAL_162>', '<SPECIAL_163>', '<SPECIAL_164>', '<SPECIAL_165>', '<SPECIAL_166>', '<SPECIAL_167>', '<SPECIAL_168>', '<SPECIAL_169>', '<SPECIAL_170>', '<SPECIAL_171>', '<SPECIAL_172>', '<SPECIAL_173>', '<SPECIAL_174>', '<SPECIAL_175>', '<SPECIAL_176>', '<SPECIAL_177>', '<SPECIAL_178>', '<SPECIAL_179>', '<SPECIAL_180>', '<SPECIAL_181>', '<SPECIAL_182>', '<SPECIAL_183>', '<SPECIAL_184>', '<SPECIAL_185>', '<SPECIAL_186>', '<SPECIAL_187>', '<SPECIAL_188>', '<SPECIAL_189>', '<SPECIAL_190>', '<SPECIAL_191>', '<SPECIAL_192>', '<SPECIAL_193>', '<SPECIAL_194>', '<SPECIAL_195>', '<SPECIAL_196>', '<SPECIAL_197>', '<SPECIAL_198>', '<SPECIAL_199>', '<SPECIAL_200>', '<SPECIAL_201>', '<SPECIAL_202>', '<SPECIAL_203>', '<SPECIAL_204>', '<SPECIAL_205>', '<SPECIAL_206>', '<SPECIAL_207>', '<SPECIAL_208>', '<SPECIAL_209>', '<SPECIAL_210>', '<SPECIAL_211>', '<SPECIAL_212>', '<SPECIAL_213>', '<SPECIAL_214>', '<SPECIAL_215>', '<SPECIAL_216>', '<SPECIAL_217>', '<SPECIAL_218>', '<SPECIAL_219>', '<SPECIAL_220>', '<SPECIAL_221>', '<SPECIAL_222>', '<SPECIAL_223>', '<SPECIAL_224>', '<SPECIAL_225>', '<SPECIAL_226>', '<SPECIAL_227>', '<SPECIAL_228>', '<SPECIAL_229>', '<SPECIAL_230>', '<SPECIAL_231>', '<SPECIAL_232>', '<SPECIAL_233>', '<SPECIAL_234>', '<SPECIAL_235>', '<SPECIAL_236>', '<SPECIAL_237>', '<SPECIAL_238>', '<SPECIAL_239>', '<SPECIAL_240>', '<SPECIAL_241>', '<SPECIAL_242>', '<SPECIAL_243>', '<SPECIAL_244>', '<SPECIAL_245>', '<SPECIAL_246>', '<SPECIAL_247>', '<SPECIAL_248>', '<SPECIAL_249>', '<SPECIAL_250>', '<SPECIAL_251>', '<SPECIAL_252>', '<SPECIAL_253>', '<SPECIAL_254>', '<SPECIAL_255>', '<SPECIAL_256>', '<SPECIAL_257>', '<SPECIAL_258>', '<SPECIAL_259>', '<SPECIAL_260>', '<SPECIAL_261>', '<SPECIAL_262>', '<SPECIAL_263>', '<SPECIAL_264>', '<SPECIAL_265>', '<SPECIAL_266>', '<SPECIAL_267>', '<SPECIAL_268>', '<SPECIAL_269>', '<SPECIAL_270>', '<SPECIAL_271>', '<SPECIAL_272>', '<SPECIAL_273>', '<SPECIAL_274>', '<SPECIAL_275>', '<SPECIAL_276>', '<SPECIAL_277>', '<SPECIAL_278>', '<SPECIAL_279>', '<SPECIAL_280>', '<SPECIAL_281>', '<SPECIAL_282>', '<SPECIAL_283>', '<SPECIAL_284>', '<SPECIAL_285>', '<SPECIAL_286>', '<SPECIAL_287>', '<SPECIAL_288>', '<SPECIAL_289>', '<SPECIAL_290>', '<SPECIAL_291>', '<SPECIAL_292>', '<SPECIAL_293>', '<SPECIAL_294>', '<SPECIAL_295>', '<SPECIAL_296>', '<SPECIAL_297>', '<SPECIAL_298>', '<SPECIAL_299>', '<SPECIAL_300>', '<SPECIAL_301>', '<SPECIAL_302>', '<SPECIAL_303>', '<SPECIAL_304>', '<SPECIAL_305>', '<SPECIAL_306>', '<SPECIAL_307>', '<SPECIAL_308>', '<SPECIAL_309>', '<SPECIAL_310>', '<SPECIAL_311>', '<SPECIAL_312>', '<SPECIAL_313>', '<SPECIAL_314>', '<SPECIAL_315>', '<SPECIAL_316>', '<SPECIAL_317>', '<SPECIAL_318>', '<SPECIAL_319>', '<SPECIAL_320>', '<SPECIAL_321>', '<SPECIAL_322>', '<SPECIAL_323>', '<SPECIAL_324>', '<SPECIAL_325>', '<SPECIAL_326>', '<SPECIAL_327>', '<SPECIAL_328>', '<SPECIAL_329>', '<SPECIAL_330>', '<SPECIAL_331>', '<SPECIAL_332>', '<SPECIAL_333>', '<SPECIAL_334>', '<SPECIAL_335>', '<SPECIAL_336>', '<SPECIAL_337>', '<SPECIAL_338>', '<SPECIAL_339>', '<SPECIAL_340>', '<SPECIAL_341>', '<SPECIAL_342>', '<SPECIAL_343>', '<SPECIAL_344>', '<SPECIAL_345>', '<SPECIAL_346>', '<SPECIAL_347>', '<SPECIAL_348>', '<SPECIAL_349>', '<SPECIAL_350>', '<SPECIAL_351>', '<SPECIAL_352>', '<SPECIAL_353>', '<SPECIAL_354>', '<SPECIAL_355>', '<SPECIAL_356>', '<SPECIAL_357>', '<SPECIAL_358>', '<SPECIAL_359>', '<SPECIAL_360>', '<SPECIAL_361>', '<SPECIAL_362>', '<SPECIAL_363>', '<SPECIAL_364>', '<SPECIAL_365>', '<SPECIAL_366>', '<SPECIAL_367>', '<SPECIAL_368>', '<SPECIAL_369>', '<SPECIAL_370>', '<SPECIAL_371>', '<SPECIAL_372>', '<SPECIAL_373>', '<SPECIAL_374>', '<SPECIAL_375>', '<SPECIAL_376>', '<SPECIAL_377>', '<SPECIAL_378>', '<SPECIAL_379>', '<SPECIAL_380>', '<SPECIAL_381>', '<SPECIAL_382>', '<SPECIAL_383>', '<SPECIAL_384>', '<SPECIAL_385>', '<SPECIAL_386>', '<SPECIAL_387>', '<SPECIAL_388>', '<SPECIAL_389>', '<SPECIAL_390>', '<SPECIAL_391>', '<SPECIAL_392>', '<SPECIAL_393>', '<SPECIAL_394>', '<SPECIAL_395>', '<SPECIAL_396>', '<SPECIAL_397>', '<SPECIAL_398>', '<SPECIAL_399>', '<SPECIAL_400>', '<SPECIAL_401>', '<SPECIAL_402>', '<SPECIAL_403>', '<SPECIAL_404>', '<SPECIAL_405>', '<SPECIAL_406>', '<SPECIAL_407>', '<SPECIAL_408>', '<SPECIAL_409>', '<SPECIAL_410>', '<SPECIAL_411>', '<SPECIAL_412>', '<SPECIAL_413>', '<SPECIAL_414>', '<SPECIAL_415>', '<SPECIAL_416>', '<SPECIAL_417>', '<SPECIAL_418>', '<SPECIAL_419>', '<SPECIAL_420>', '<SPECIAL_421>', '<SPECIAL_422>', '<SPECIAL_423>', '<SPECIAL_424>', '<SPECIAL_425>', '<SPECIAL_426>', '<SPECIAL_427>', '<SPECIAL_428>', '<SPECIAL_429>', '<SPECIAL_430>', '<SPECIAL_431>', '<SPECIAL_432>', '<SPECIAL_433>', '<SPECIAL_434>', '<SPECIAL_435>', '<SPECIAL_436>', '<SPECIAL_437>', '<SPECIAL_438>', '<SPECIAL_439>', '<SPECIAL_440>', '<SPECIAL_441>', '<SPECIAL_442>', '<SPECIAL_443>', '<SPECIAL_444>', '<SPECIAL_445>', '<SPECIAL_446>', '<SPECIAL_447>', '<SPECIAL_448>', '<SPECIAL_449>', '<SPECIAL_450>', '<SPECIAL_451>', '<SPECIAL_452>', '<SPECIAL_453>', '<SPECIAL_454>', '<SPECIAL_455>', '<SPECIAL_456>', '<SPECIAL_457>', '<SPECIAL_458>', '<SPECIAL_459>', '<SPECIAL_460>', '<SPECIAL_461>', '<SPECIAL_462>', '<SPECIAL_463>', '<SPECIAL_464>', '<SPECIAL_465>', '<SPECIAL_466>', '<SPECIAL_467>', '<SPECIAL_468>', '<SPECIAL_469>', '<SPECIAL_470>', '<SPECIAL_471>', '<SPECIAL_472>', '<SPECIAL_473>', '<SPECIAL_474>', '<SPECIAL_475>', '<SPECIAL_476>', '<SPECIAL_477>', '<SPECIAL_478>', '<SPECIAL_479>', '<SPECIAL_480>', '<SPECIAL_481>', '<SPECIAL_482>', '<SPECIAL_483>', '<SPECIAL_484>', '<SPECIAL_485>', '<SPECIAL_486>', '<SPECIAL_487>', '<SPECIAL_488>', '<SPECIAL_489>', '<SPECIAL_490>', '<SPECIAL_491>', '<SPECIAL_492>', '<SPECIAL_493>', '<SPECIAL_494>', '<SPECIAL_495>', '<SPECIAL_496>', '<SPECIAL_497>', '<SPECIAL_498>', '<SPECIAL_499>', '<SPECIAL_500>', '<SPECIAL_501>', '<SPECIAL_502>', '<SPECIAL_503>', '<SPECIAL_504>', '<SPECIAL_505>', '<SPECIAL_506>', '<SPECIAL_507>', '<SPECIAL_508>', '<SPECIAL_509>', '<SPECIAL_510>', '<SPECIAL_511>', '<SPECIAL_512>', '<SPECIAL_513>', '<SPECIAL_514>', '<SPECIAL_515>', '<SPECIAL_516>', '<SPECIAL_517>', '<SPECIAL_518>', '<SPECIAL_519>', '<SPECIAL_520>', '<SPECIAL_521>', '<SPECIAL_522>', '<SPECIAL_523>', '<SPECIAL_524>', '<SPECIAL_525>', '<SPECIAL_526>', '<SPECIAL_527>', '<SPECIAL_528>', '<SPECIAL_529>', '<SPECIAL_530>', '<SPECIAL_531>', '<SPECIAL_532>', '<SPECIAL_533>', '<SPECIAL_534>', '<SPECIAL_535>', '<SPECIAL_536>', '<SPECIAL_537>', '<SPECIAL_538>', '<SPECIAL_539>', '<SPECIAL_540>', '<SPECIAL_541>', '<SPECIAL_542>', '<SPECIAL_543>', '<SPECIAL_544>', '<SPECIAL_545>', '<SPECIAL_546>', '<SPECIAL_547>', '<SPECIAL_548>', '<SPECIAL_549>', '<SPECIAL_550>', '<SPECIAL_551>', '<SPECIAL_552>', '<SPECIAL_553>', '<SPECIAL_554>', '<SPECIAL_555>', '<SPECIAL_556>', '<SPECIAL_557>', '<SPECIAL_558>', '<SPECIAL_559>', '<SPECIAL_560>', '<SPECIAL_561>', '<SPECIAL_562>', '<SPECIAL_563>', '<SPECIAL_564>', '<SPECIAL_565>', '<SPECIAL_566>', '<SPECIAL_567>', '<SPECIAL_568>', '<SPECIAL_569>', '<SPECIAL_570>', '<SPECIAL_571>', '<SPECIAL_572>', '<SPECIAL_573>', '<SPECIAL_574>', '<SPECIAL_575>', '<SPECIAL_576>', '<SPECIAL_577>', '<SPECIAL_578>', '<SPECIAL_579>', '<SPECIAL_580>', '<SPECIAL_581>', '<SPECIAL_582>', '<SPECIAL_583>', '<SPECIAL_584>', '<SPECIAL_585>', '<SPECIAL_586>', '<SPECIAL_587>', '<SPECIAL_588>', '<SPECIAL_589>', '<SPECIAL_590>', '<SPECIAL_591>', '<SPECIAL_592>', '<SPECIAL_593>', '<SPECIAL_594>', '<SPECIAL_595>', '<SPECIAL_596>', '<SPECIAL_597>', '<SPECIAL_598>', '<SPECIAL_599>', '<SPECIAL_600>', '<SPECIAL_601>', '<SPECIAL_602>', '<SPECIAL_603>', '<SPECIAL_604>', '<SPECIAL_605>', '<SPECIAL_606>', '<SPECIAL_607>', '<SPECIAL_608>', '<SPECIAL_609>', '<SPECIAL_610>', '<SPECIAL_611>', '<SPECIAL_612>', '<SPECIAL_613>', '<SPECIAL_614>', '<SPECIAL_615>', '<SPECIAL_616>', '<SPECIAL_617>', '<SPECIAL_618>', '<SPECIAL_619>', '<SPECIAL_620>', '<SPECIAL_621>', '<SPECIAL_622>', '<SPECIAL_623>', '<SPECIAL_624>', '<SPECIAL_625>', '<SPECIAL_626>', '<SPECIAL_627>', '<SPECIAL_628>', '<SPECIAL_629>', '<SPECIAL_630>', '<SPECIAL_631>', '<SPECIAL_632>', '<SPECIAL_633>', '<SPECIAL_634>', '<SPECIAL_635>', '<SPECIAL_636>', '<SPECIAL_637>', '<SPECIAL_638>', '<SPECIAL_639>', '<SPECIAL_640>', '<SPECIAL_641>', '<SPECIAL_642>', '<SPECIAL_643>', '<SPECIAL_644>', '<SPECIAL_645>', '<SPECIAL_646>', '<SPECIAL_647>', '<SPECIAL_648>', '<SPECIAL_649>', '<SPECIAL_650>', '<SPECIAL_651>', '<SPECIAL_652>', '<SPECIAL_653>', '<SPECIAL_654>', '<SPECIAL_655>', '<SPECIAL_656>', '<SPECIAL_657>', '<SPECIAL_658>', '<SPECIAL_659>', '<SPECIAL_660>', '<SPECIAL_661>', '<SPECIAL_662>', '<SPECIAL_663>', '<SPECIAL_664>', '<SPECIAL_665>', '<SPECIAL_666>', '<SPECIAL_667>', '<SPECIAL_668>', '<SPECIAL_669>', '<SPECIAL_670>', '<SPECIAL_671>', '<SPECIAL_672>', '<SPECIAL_673>', '<SPECIAL_674>', '<SPECIAL_675>', '<SPECIAL_676>', '<SPECIAL_677>', '<SPECIAL_678>', '<SPECIAL_679>', '<SPECIAL_680>', '<SPECIAL_681>', '<SPECIAL_682>', '<SPECIAL_683>', '<SPECIAL_684>', '<SPECIAL_685>', '<SPECIAL_686>', '<SPECIAL_687>', '<SPECIAL_688>', '<SPECIAL_689>', '<SPECIAL_690>', '<SPECIAL_691>', '<SPECIAL_692>', '<SPECIAL_693>', '<SPECIAL_694>', '<SPECIAL_695>', '<SPECIAL_696>', '<SPECIAL_697>', '<SPECIAL_698>', '<SPECIAL_699>', '<SPECIAL_700>', '<SPECIAL_701>', '<SPECIAL_702>', '<SPECIAL_703>', '<SPECIAL_704>', '<SPECIAL_705>', '<SPECIAL_706>', '<SPECIAL_707>', '<SPECIAL_708>', '<SPECIAL_709>', '<SPECIAL_710>', '<SPECIAL_711>', '<SPECIAL_712>', '<SPECIAL_713>', '<SPECIAL_714>', '<SPECIAL_715>', '<SPECIAL_716>', '<SPECIAL_717>', '<SPECIAL_718>', '<SPECIAL_719>', '<SPECIAL_720>', '<SPECIAL_721>', '<SPECIAL_722>', '<SPECIAL_723>', '<SPECIAL_724>', '<SPECIAL_725>', '<SPECIAL_726>', '<SPECIAL_727>', '<SPECIAL_728>', '<SPECIAL_729>', '<SPECIAL_730>', '<SPECIAL_731>', '<SPECIAL_732>', '<SPECIAL_733>', '<SPECIAL_734>', '<SPECIAL_735>', '<SPECIAL_736>', '<SPECIAL_737>', '<SPECIAL_738>', '<SPECIAL_739>', '<SPECIAL_740>', '<SPECIAL_741>', '<SPECIAL_742>', '<SPECIAL_743>', '<SPECIAL_744>', '<SPECIAL_745>', '<SPECIAL_746>', '<SPECIAL_747>', '<SPECIAL_748>', '<SPECIAL_749>', '<SPECIAL_750>', '<SPECIAL_751>', '<SPECIAL_752>', '<SPECIAL_753>', '<SPECIAL_754>', '<SPECIAL_755>', '<SPECIAL_756>', '<SPECIAL_757>', '<SPECIAL_758>', '<SPECIAL_759>', '<SPECIAL_760>', '<SPECIAL_761>', '<SPECIAL_762>', '<SPECIAL_763>', '<SPECIAL_764>', '<SPECIAL_765>', '<SPECIAL_766>', '<SPECIAL_767>', '<SPECIAL_768>', '<SPECIAL_769>', '<SPECIAL_770>', '<SPECIAL_771>', '<SPECIAL_772>', '<SPECIAL_773>', '<SPECIAL_774>', '<SPECIAL_775>', '<SPECIAL_776>', '<SPECIAL_777>', '<SPECIAL_778>', '<SPECIAL_779>', '<SPECIAL_780>', '<SPECIAL_781>', '<SPECIAL_782>', '<SPECIAL_783>', '<SPECIAL_784>', '<SPECIAL_785>', '<SPECIAL_786>', '<SPECIAL_787>', '<SPECIAL_788>', '<SPECIAL_789>', '<SPECIAL_790>', '<SPECIAL_791>', '<SPECIAL_792>', '<SPECIAL_793>', '<SPECIAL_794>', '<SPECIAL_795>', '<SPECIAL_796>', '<SPECIAL_797>', '<SPECIAL_798>', '<SPECIAL_799>', '<SPECIAL_800>', '<SPECIAL_801>', '<SPECIAL_802>', '<SPECIAL_803>', '<SPECIAL_804>', '<SPECIAL_805>', '<SPECIAL_806>', '<SPECIAL_807>', '<SPECIAL_808>', '<SPECIAL_809>', '<SPECIAL_810>', '<SPECIAL_811>', '<SPECIAL_812>', '<SPECIAL_813>', '<SPECIAL_814>', '<SPECIAL_815>', '<SPECIAL_816>', '<SPECIAL_817>', '<SPECIAL_818>', '<SPECIAL_819>', '<SPECIAL_820>', '<SPECIAL_821>', '<SPECIAL_822>', '<SPECIAL_823>', '<SPECIAL_824>', '<SPECIAL_825>', '<SPECIAL_826>', '<SPECIAL_827>', '<SPECIAL_828>', '<SPECIAL_829>', '<SPECIAL_830>', '<SPECIAL_831>', '<SPECIAL_832>', '<SPECIAL_833>', '<SPECIAL_834>', '<SPECIAL_835>', '<SPECIAL_836>', '<SPECIAL_837>', '<SPECIAL_838>', '<SPECIAL_839>', '<SPECIAL_840>', '<SPECIAL_841>', '<SPECIAL_842>', '<SPECIAL_843>', '<SPECIAL_844>', '<SPECIAL_845>', '<SPECIAL_846>', '<SPECIAL_847>', '<SPECIAL_848>', '<SPECIAL_849>', '<SPECIAL_850>', '<SPECIAL_851>', '<SPECIAL_852>', '<SPECIAL_853>', '<SPECIAL_854>', '<SPECIAL_855>', '<SPECIAL_856>', '<SPECIAL_857>', '<SPECIAL_858>', '<SPECIAL_859>', '<SPECIAL_860>', '<SPECIAL_861>', '<SPECIAL_862>', '<SPECIAL_863>', '<SPECIAL_864>', '<SPECIAL_865>', '<SPECIAL_866>', '<SPECIAL_867>', '<SPECIAL_868>', '<SPECIAL_869>', '<SPECIAL_870>', '<SPECIAL_871>', '<SPECIAL_872>', '<SPECIAL_873>', '<SPECIAL_874>', '<SPECIAL_875>', '<SPECIAL_876>', '<SPECIAL_877>', '<SPECIAL_878>', '<SPECIAL_879>', '<SPECIAL_880>', '<SPECIAL_881>', '<SPECIAL_882>', '<SPECIAL_883>', '<SPECIAL_884>', '<SPECIAL_885>', '<SPECIAL_886>', '<SPECIAL_887>', '<SPECIAL_888>', '<SPECIAL_889>', '<SPECIAL_890>', '<SPECIAL_891>', '<SPECIAL_892>', '<SPECIAL_893>', '<SPECIAL_894>', '<SPECIAL_895>', '<SPECIAL_896>', '<SPECIAL_897>', '<SPECIAL_898>', '<SPECIAL_899>', '<SPECIAL_900>', '<SPECIAL_901>', '<SPECIAL_902>', '<SPECIAL_903>', '<SPECIAL_904>', '<SPECIAL_905>', '<SPECIAL_906>', '<SPECIAL_907>', '<SPECIAL_908>', '<SPECIAL_909>', '<SPECIAL_910>', '<SPECIAL_911>', '<SPECIAL_912>', '<SPECIAL_913>', '<SPECIAL_914>', '<SPECIAL_915>', '<SPECIAL_916>', '<SPECIAL_917>', '<SPECIAL_918>', '<SPECIAL_919>', '<SPECIAL_920>', '<SPECIAL_921>', '<SPECIAL_922>', '<SPECIAL_923>', '<SPECIAL_924>', '<SPECIAL_925>', '<SPECIAL_926>', '<SPECIAL_927>', '<SPECIAL_928>', '<SPECIAL_929>', '<SPECIAL_930>', '<SPECIAL_931>', '<SPECIAL_932>', '<SPECIAL_933>', '<SPECIAL_934>', '<SPECIAL_935>', '<SPECIAL_936>', '<SPECIAL_937>', '<SPECIAL_938>', '<SPECIAL_939>', '<SPECIAL_940>', '<SPECIAL_941>', '<SPECIAL_942>', '<SPECIAL_943>', '<SPECIAL_944>', '<SPECIAL_945>', '<SPECIAL_946>', '<SPECIAL_947>', '<SPECIAL_948>', '<SPECIAL_949>', '<SPECIAL_950>', '<SPECIAL_951>', '<SPECIAL_952>', '<SPECIAL_953>', '<SPECIAL_954>', '<SPECIAL_955>', '<SPECIAL_956>', '<SPECIAL_957>', '<SPECIAL_958>', '<SPECIAL_959>', '<SPECIAL_960>', '<SPECIAL_961>', '<SPECIAL_962>', '<SPECIAL_963>', '<SPECIAL_964>', '<SPECIAL_965>', '<SPECIAL_966>', '<SPECIAL_967>', '<SPECIAL_968>', '<SPECIAL_969>', '<SPECIAL_970>', '<SPECIAL_971>', '<SPECIAL_972>', '<SPECIAL_973>', '<SPECIAL_974>', '<SPECIAL_975>', '<SPECIAL_976>', '<SPECIAL_977>', '<SPECIAL_978>', '<SPECIAL_979>', '<SPECIAL_980>', '<SPECIAL_981>', '<SPECIAL_982>', '<SPECIAL_983>', '<SPECIAL_984>', '<SPECIAL_985>', '<SPECIAL_986>', '<SPECIAL_987>', '<SPECIAL_988>', '<SPECIAL_989>', '<SPECIAL_990>', '<SPECIAL_991>', '<SPECIAL_992>', '<SPECIAL_993>', '<SPECIAL_994>', '<SPECIAL_995>', '<SPECIAL_996>', '<SPECIAL_997>', '<SPECIAL_998>', '<SPECIAL_999>']}\n",
            "All added tokens: ['<unk>', '<s>', '</s>', '[INST]', '[/INST]', '[AVAILABLE_TOOLS]', '[/AVAILABLE_TOOLS]', '[TOOL_RESULTS]', '[/TOOL_RESULTS]', '[TOOL_CALLS]', '[IMG]', '<pad>', '[IMG_BREAK]', '[IMG_END]', '[PREFIX]', '[MIDDLE]', '[SUFFIX]', '[SYSTEM_PROMPT]', '[/SYSTEM_PROMPT]', '[TOOL_CONTENT]', '<SPECIAL_20>', '<SPECIAL_21>', '<SPECIAL_22>', '<SPECIAL_23>', '<SPECIAL_24>', '<SPECIAL_25>', '<SPECIAL_26>', '<SPECIAL_27>', '<SPECIAL_28>', '<SPECIAL_29>', '<SPECIAL_30>', '<SPECIAL_31>', '<SPECIAL_32>', '<SPECIAL_33>', '<SPECIAL_34>', '<SPECIAL_35>', '<SPECIAL_36>', '<SPECIAL_37>', '<SPECIAL_38>', '<SPECIAL_39>', '<SPECIAL_40>', '<SPECIAL_41>', '<SPECIAL_42>', '<SPECIAL_43>', '<SPECIAL_44>', '<SPECIAL_45>', '<SPECIAL_46>', '<SPECIAL_47>', '<SPECIAL_48>', '<SPECIAL_49>', '<SPECIAL_50>', '<SPECIAL_51>', '<SPECIAL_52>', '<SPECIAL_53>', '<SPECIAL_54>', '<SPECIAL_55>', '<SPECIAL_56>', '<SPECIAL_57>', '<SPECIAL_58>', '<SPECIAL_59>', '<SPECIAL_60>', '<SPECIAL_61>', '<SPECIAL_62>', '<SPECIAL_63>', '<SPECIAL_64>', '<SPECIAL_65>', '<SPECIAL_66>', '<SPECIAL_67>', '<SPECIAL_68>', '<SPECIAL_69>', '<SPECIAL_70>', '<SPECIAL_71>', '<SPECIAL_72>', '<SPECIAL_73>', '<SPECIAL_74>', '<SPECIAL_75>', '<SPECIAL_76>', '<SPECIAL_77>', '<SPECIAL_78>', '<SPECIAL_79>', '<SPECIAL_80>', '<SPECIAL_81>', '<SPECIAL_82>', '<SPECIAL_83>', '<SPECIAL_84>', '<SPECIAL_85>', '<SPECIAL_86>', '<SPECIAL_87>', '<SPECIAL_88>', '<SPECIAL_89>', '<SPECIAL_90>', '<SPECIAL_91>', '<SPECIAL_92>', '<SPECIAL_93>', '<SPECIAL_94>', '<SPECIAL_95>', '<SPECIAL_96>', '<SPECIAL_97>', '<SPECIAL_98>', '<SPECIAL_99>', '<SPECIAL_100>', '<SPECIAL_101>', '<SPECIAL_102>', '<SPECIAL_103>', '<SPECIAL_104>', '<SPECIAL_105>', '<SPECIAL_106>', '<SPECIAL_107>', '<SPECIAL_108>', '<SPECIAL_109>', '<SPECIAL_110>', '<SPECIAL_111>', '<SPECIAL_112>', '<SPECIAL_113>', '<SPECIAL_114>', '<SPECIAL_115>', '<SPECIAL_116>', '<SPECIAL_117>', '<SPECIAL_118>', '<SPECIAL_119>', '<SPECIAL_120>', '<SPECIAL_121>', '<SPECIAL_122>', '<SPECIAL_123>', '<SPECIAL_124>', '<SPECIAL_125>', '<SPECIAL_126>', '<SPECIAL_127>', '<SPECIAL_128>', '<SPECIAL_129>', '<SPECIAL_130>', '<SPECIAL_131>', '<SPECIAL_132>', '<SPECIAL_133>', '<SPECIAL_134>', '<SPECIAL_135>', '<SPECIAL_136>', '<SPECIAL_137>', '<SPECIAL_138>', '<SPECIAL_139>', '<SPECIAL_140>', '<SPECIAL_141>', '<SPECIAL_142>', '<SPECIAL_143>', '<SPECIAL_144>', '<SPECIAL_145>', '<SPECIAL_146>', '<SPECIAL_147>', '<SPECIAL_148>', '<SPECIAL_149>', '<SPECIAL_150>', '<SPECIAL_151>', '<SPECIAL_152>', '<SPECIAL_153>', '<SPECIAL_154>', '<SPECIAL_155>', '<SPECIAL_156>', '<SPECIAL_157>', '<SPECIAL_158>', '<SPECIAL_159>', '<SPECIAL_160>', '<SPECIAL_161>', '<SPECIAL_162>', '<SPECIAL_163>', '<SPECIAL_164>', '<SPECIAL_165>', '<SPECIAL_166>', '<SPECIAL_167>', '<SPECIAL_168>', '<SPECIAL_169>', '<SPECIAL_170>', '<SPECIAL_171>', '<SPECIAL_172>', '<SPECIAL_173>', '<SPECIAL_174>', '<SPECIAL_175>', '<SPECIAL_176>', '<SPECIAL_177>', '<SPECIAL_178>', '<SPECIAL_179>', '<SPECIAL_180>', '<SPECIAL_181>', '<SPECIAL_182>', '<SPECIAL_183>', '<SPECIAL_184>', '<SPECIAL_185>', '<SPECIAL_186>', '<SPECIAL_187>', '<SPECIAL_188>', '<SPECIAL_189>', '<SPECIAL_190>', '<SPECIAL_191>', '<SPECIAL_192>', '<SPECIAL_193>', '<SPECIAL_194>', '<SPECIAL_195>', '<SPECIAL_196>', '<SPECIAL_197>', '<SPECIAL_198>', '<SPECIAL_199>', '<SPECIAL_200>', '<SPECIAL_201>', '<SPECIAL_202>', '<SPECIAL_203>', '<SPECIAL_204>', '<SPECIAL_205>', '<SPECIAL_206>', '<SPECIAL_207>', '<SPECIAL_208>', '<SPECIAL_209>', '<SPECIAL_210>', '<SPECIAL_211>', '<SPECIAL_212>', '<SPECIAL_213>', '<SPECIAL_214>', '<SPECIAL_215>', '<SPECIAL_216>', '<SPECIAL_217>', '<SPECIAL_218>', '<SPECIAL_219>', '<SPECIAL_220>', '<SPECIAL_221>', '<SPECIAL_222>', '<SPECIAL_223>', '<SPECIAL_224>', '<SPECIAL_225>', '<SPECIAL_226>', '<SPECIAL_227>', '<SPECIAL_228>', '<SPECIAL_229>', '<SPECIAL_230>', '<SPECIAL_231>', '<SPECIAL_232>', '<SPECIAL_233>', '<SPECIAL_234>', '<SPECIAL_235>', '<SPECIAL_236>', '<SPECIAL_237>', '<SPECIAL_238>', '<SPECIAL_239>', '<SPECIAL_240>', '<SPECIAL_241>', '<SPECIAL_242>', '<SPECIAL_243>', '<SPECIAL_244>', '<SPECIAL_245>', '<SPECIAL_246>', '<SPECIAL_247>', '<SPECIAL_248>', '<SPECIAL_249>', '<SPECIAL_250>', '<SPECIAL_251>', '<SPECIAL_252>', '<SPECIAL_253>', '<SPECIAL_254>', '<SPECIAL_255>', '<SPECIAL_256>', '<SPECIAL_257>', '<SPECIAL_258>', '<SPECIAL_259>', '<SPECIAL_260>', '<SPECIAL_261>', '<SPECIAL_262>', '<SPECIAL_263>', '<SPECIAL_264>', '<SPECIAL_265>', '<SPECIAL_266>', '<SPECIAL_267>', '<SPECIAL_268>', '<SPECIAL_269>', '<SPECIAL_270>', '<SPECIAL_271>', '<SPECIAL_272>', '<SPECIAL_273>', '<SPECIAL_274>', '<SPECIAL_275>', '<SPECIAL_276>', '<SPECIAL_277>', '<SPECIAL_278>', '<SPECIAL_279>', '<SPECIAL_280>', '<SPECIAL_281>', '<SPECIAL_282>', '<SPECIAL_283>', '<SPECIAL_284>', '<SPECIAL_285>', '<SPECIAL_286>', '<SPECIAL_287>', '<SPECIAL_288>', '<SPECIAL_289>', '<SPECIAL_290>', '<SPECIAL_291>', '<SPECIAL_292>', '<SPECIAL_293>', '<SPECIAL_294>', '<SPECIAL_295>', '<SPECIAL_296>', '<SPECIAL_297>', '<SPECIAL_298>', '<SPECIAL_299>', '<SPECIAL_300>', '<SPECIAL_301>', '<SPECIAL_302>', '<SPECIAL_303>', '<SPECIAL_304>', '<SPECIAL_305>', '<SPECIAL_306>', '<SPECIAL_307>', '<SPECIAL_308>', '<SPECIAL_309>', '<SPECIAL_310>', '<SPECIAL_311>', '<SPECIAL_312>', '<SPECIAL_313>', '<SPECIAL_314>', '<SPECIAL_315>', '<SPECIAL_316>', '<SPECIAL_317>', '<SPECIAL_318>', '<SPECIAL_319>', '<SPECIAL_320>', '<SPECIAL_321>', '<SPECIAL_322>', '<SPECIAL_323>', '<SPECIAL_324>', '<SPECIAL_325>', '<SPECIAL_326>', '<SPECIAL_327>', '<SPECIAL_328>', '<SPECIAL_329>', '<SPECIAL_330>', '<SPECIAL_331>', '<SPECIAL_332>', '<SPECIAL_333>', '<SPECIAL_334>', '<SPECIAL_335>', '<SPECIAL_336>', '<SPECIAL_337>', '<SPECIAL_338>', '<SPECIAL_339>', '<SPECIAL_340>', '<SPECIAL_341>', '<SPECIAL_342>', '<SPECIAL_343>', '<SPECIAL_344>', '<SPECIAL_345>', '<SPECIAL_346>', '<SPECIAL_347>', '<SPECIAL_348>', '<SPECIAL_349>', '<SPECIAL_350>', '<SPECIAL_351>', '<SPECIAL_352>', '<SPECIAL_353>', '<SPECIAL_354>', '<SPECIAL_355>', '<SPECIAL_356>', '<SPECIAL_357>', '<SPECIAL_358>', '<SPECIAL_359>', '<SPECIAL_360>', '<SPECIAL_361>', '<SPECIAL_362>', '<SPECIAL_363>', '<SPECIAL_364>', '<SPECIAL_365>', '<SPECIAL_366>', '<SPECIAL_367>', '<SPECIAL_368>', '<SPECIAL_369>', '<SPECIAL_370>', '<SPECIAL_371>', '<SPECIAL_372>', '<SPECIAL_373>', '<SPECIAL_374>', '<SPECIAL_375>', '<SPECIAL_376>', '<SPECIAL_377>', '<SPECIAL_378>', '<SPECIAL_379>', '<SPECIAL_380>', '<SPECIAL_381>', '<SPECIAL_382>', '<SPECIAL_383>', '<SPECIAL_384>', '<SPECIAL_385>', '<SPECIAL_386>', '<SPECIAL_387>', '<SPECIAL_388>', '<SPECIAL_389>', '<SPECIAL_390>', '<SPECIAL_391>', '<SPECIAL_392>', '<SPECIAL_393>', '<SPECIAL_394>', '<SPECIAL_395>', '<SPECIAL_396>', '<SPECIAL_397>', '<SPECIAL_398>', '<SPECIAL_399>', '<SPECIAL_400>', '<SPECIAL_401>', '<SPECIAL_402>', '<SPECIAL_403>', '<SPECIAL_404>', '<SPECIAL_405>', '<SPECIAL_406>', '<SPECIAL_407>', '<SPECIAL_408>', '<SPECIAL_409>', '<SPECIAL_410>', '<SPECIAL_411>', '<SPECIAL_412>', '<SPECIAL_413>', '<SPECIAL_414>', '<SPECIAL_415>', '<SPECIAL_416>', '<SPECIAL_417>', '<SPECIAL_418>', '<SPECIAL_419>', '<SPECIAL_420>', '<SPECIAL_421>', '<SPECIAL_422>', '<SPECIAL_423>', '<SPECIAL_424>', '<SPECIAL_425>', '<SPECIAL_426>', '<SPECIAL_427>', '<SPECIAL_428>', '<SPECIAL_429>', '<SPECIAL_430>', '<SPECIAL_431>', '<SPECIAL_432>', '<SPECIAL_433>', '<SPECIAL_434>', '<SPECIAL_435>', '<SPECIAL_436>', '<SPECIAL_437>', '<SPECIAL_438>', '<SPECIAL_439>', '<SPECIAL_440>', '<SPECIAL_441>', '<SPECIAL_442>', '<SPECIAL_443>', '<SPECIAL_444>', '<SPECIAL_445>', '<SPECIAL_446>', '<SPECIAL_447>', '<SPECIAL_448>', '<SPECIAL_449>', '<SPECIAL_450>', '<SPECIAL_451>', '<SPECIAL_452>', '<SPECIAL_453>', '<SPECIAL_454>', '<SPECIAL_455>', '<SPECIAL_456>', '<SPECIAL_457>', '<SPECIAL_458>', '<SPECIAL_459>', '<SPECIAL_460>', '<SPECIAL_461>', '<SPECIAL_462>', '<SPECIAL_463>', '<SPECIAL_464>', '<SPECIAL_465>', '<SPECIAL_466>', '<SPECIAL_467>', '<SPECIAL_468>', '<SPECIAL_469>', '<SPECIAL_470>', '<SPECIAL_471>', '<SPECIAL_472>', '<SPECIAL_473>', '<SPECIAL_474>', '<SPECIAL_475>', '<SPECIAL_476>', '<SPECIAL_477>', '<SPECIAL_478>', '<SPECIAL_479>', '<SPECIAL_480>', '<SPECIAL_481>', '<SPECIAL_482>', '<SPECIAL_483>', '<SPECIAL_484>', '<SPECIAL_485>', '<SPECIAL_486>', '<SPECIAL_487>', '<SPECIAL_488>', '<SPECIAL_489>', '<SPECIAL_490>', '<SPECIAL_491>', '<SPECIAL_492>', '<SPECIAL_493>', '<SPECIAL_494>', '<SPECIAL_495>', '<SPECIAL_496>', '<SPECIAL_497>', '<SPECIAL_498>', '<SPECIAL_499>', '<SPECIAL_500>', '<SPECIAL_501>', '<SPECIAL_502>', '<SPECIAL_503>', '<SPECIAL_504>', '<SPECIAL_505>', '<SPECIAL_506>', '<SPECIAL_507>', '<SPECIAL_508>', '<SPECIAL_509>', '<SPECIAL_510>', '<SPECIAL_511>', '<SPECIAL_512>', '<SPECIAL_513>', '<SPECIAL_514>', '<SPECIAL_515>', '<SPECIAL_516>', '<SPECIAL_517>', '<SPECIAL_518>', '<SPECIAL_519>', '<SPECIAL_520>', '<SPECIAL_521>', '<SPECIAL_522>', '<SPECIAL_523>', '<SPECIAL_524>', '<SPECIAL_525>', '<SPECIAL_526>', '<SPECIAL_527>', '<SPECIAL_528>', '<SPECIAL_529>', '<SPECIAL_530>', '<SPECIAL_531>', '<SPECIAL_532>', '<SPECIAL_533>', '<SPECIAL_534>', '<SPECIAL_535>', '<SPECIAL_536>', '<SPECIAL_537>', '<SPECIAL_538>', '<SPECIAL_539>', '<SPECIAL_540>', '<SPECIAL_541>', '<SPECIAL_542>', '<SPECIAL_543>', '<SPECIAL_544>', '<SPECIAL_545>', '<SPECIAL_546>', '<SPECIAL_547>', '<SPECIAL_548>', '<SPECIAL_549>', '<SPECIAL_550>', '<SPECIAL_551>', '<SPECIAL_552>', '<SPECIAL_553>', '<SPECIAL_554>', '<SPECIAL_555>', '<SPECIAL_556>', '<SPECIAL_557>', '<SPECIAL_558>', '<SPECIAL_559>', '<SPECIAL_560>', '<SPECIAL_561>', '<SPECIAL_562>', '<SPECIAL_563>', '<SPECIAL_564>', '<SPECIAL_565>', '<SPECIAL_566>', '<SPECIAL_567>', '<SPECIAL_568>', '<SPECIAL_569>', '<SPECIAL_570>', '<SPECIAL_571>', '<SPECIAL_572>', '<SPECIAL_573>', '<SPECIAL_574>', '<SPECIAL_575>', '<SPECIAL_576>', '<SPECIAL_577>', '<SPECIAL_578>', '<SPECIAL_579>', '<SPECIAL_580>', '<SPECIAL_581>', '<SPECIAL_582>', '<SPECIAL_583>', '<SPECIAL_584>', '<SPECIAL_585>', '<SPECIAL_586>', '<SPECIAL_587>', '<SPECIAL_588>', '<SPECIAL_589>', '<SPECIAL_590>', '<SPECIAL_591>', '<SPECIAL_592>', '<SPECIAL_593>', '<SPECIAL_594>', '<SPECIAL_595>', '<SPECIAL_596>', '<SPECIAL_597>', '<SPECIAL_598>', '<SPECIAL_599>', '<SPECIAL_600>', '<SPECIAL_601>', '<SPECIAL_602>', '<SPECIAL_603>', '<SPECIAL_604>', '<SPECIAL_605>', '<SPECIAL_606>', '<SPECIAL_607>', '<SPECIAL_608>', '<SPECIAL_609>', '<SPECIAL_610>', '<SPECIAL_611>', '<SPECIAL_612>', '<SPECIAL_613>', '<SPECIAL_614>', '<SPECIAL_615>', '<SPECIAL_616>', '<SPECIAL_617>', '<SPECIAL_618>', '<SPECIAL_619>', '<SPECIAL_620>', '<SPECIAL_621>', '<SPECIAL_622>', '<SPECIAL_623>', '<SPECIAL_624>', '<SPECIAL_625>', '<SPECIAL_626>', '<SPECIAL_627>', '<SPECIAL_628>', '<SPECIAL_629>', '<SPECIAL_630>', '<SPECIAL_631>', '<SPECIAL_632>', '<SPECIAL_633>', '<SPECIAL_634>', '<SPECIAL_635>', '<SPECIAL_636>', '<SPECIAL_637>', '<SPECIAL_638>', '<SPECIAL_639>', '<SPECIAL_640>', '<SPECIAL_641>', '<SPECIAL_642>', '<SPECIAL_643>', '<SPECIAL_644>', '<SPECIAL_645>', '<SPECIAL_646>', '<SPECIAL_647>', '<SPECIAL_648>', '<SPECIAL_649>', '<SPECIAL_650>', '<SPECIAL_651>', '<SPECIAL_652>', '<SPECIAL_653>', '<SPECIAL_654>', '<SPECIAL_655>', '<SPECIAL_656>', '<SPECIAL_657>', '<SPECIAL_658>', '<SPECIAL_659>', '<SPECIAL_660>', '<SPECIAL_661>', '<SPECIAL_662>', '<SPECIAL_663>', '<SPECIAL_664>', '<SPECIAL_665>', '<SPECIAL_666>', '<SPECIAL_667>', '<SPECIAL_668>', '<SPECIAL_669>', '<SPECIAL_670>', '<SPECIAL_671>', '<SPECIAL_672>', '<SPECIAL_673>', '<SPECIAL_674>', '<SPECIAL_675>', '<SPECIAL_676>', '<SPECIAL_677>', '<SPECIAL_678>', '<SPECIAL_679>', '<SPECIAL_680>', '<SPECIAL_681>', '<SPECIAL_682>', '<SPECIAL_683>', '<SPECIAL_684>', '<SPECIAL_685>', '<SPECIAL_686>', '<SPECIAL_687>', '<SPECIAL_688>', '<SPECIAL_689>', '<SPECIAL_690>', '<SPECIAL_691>', '<SPECIAL_692>', '<SPECIAL_693>', '<SPECIAL_694>', '<SPECIAL_695>', '<SPECIAL_696>', '<SPECIAL_697>', '<SPECIAL_698>', '<SPECIAL_699>', '<SPECIAL_700>', '<SPECIAL_701>', '<SPECIAL_702>', '<SPECIAL_703>', '<SPECIAL_704>', '<SPECIAL_705>', '<SPECIAL_706>', '<SPECIAL_707>', '<SPECIAL_708>', '<SPECIAL_709>', '<SPECIAL_710>', '<SPECIAL_711>', '<SPECIAL_712>', '<SPECIAL_713>', '<SPECIAL_714>', '<SPECIAL_715>', '<SPECIAL_716>', '<SPECIAL_717>', '<SPECIAL_718>', '<SPECIAL_719>', '<SPECIAL_720>', '<SPECIAL_721>', '<SPECIAL_722>', '<SPECIAL_723>', '<SPECIAL_724>', '<SPECIAL_725>', '<SPECIAL_726>', '<SPECIAL_727>', '<SPECIAL_728>', '<SPECIAL_729>', '<SPECIAL_730>', '<SPECIAL_731>', '<SPECIAL_732>', '<SPECIAL_733>', '<SPECIAL_734>', '<SPECIAL_735>', '<SPECIAL_736>', '<SPECIAL_737>', '<SPECIAL_738>', '<SPECIAL_739>', '<SPECIAL_740>', '<SPECIAL_741>', '<SPECIAL_742>', '<SPECIAL_743>', '<SPECIAL_744>', '<SPECIAL_745>', '<SPECIAL_746>', '<SPECIAL_747>', '<SPECIAL_748>', '<SPECIAL_749>', '<SPECIAL_750>', '<SPECIAL_751>', '<SPECIAL_752>', '<SPECIAL_753>', '<SPECIAL_754>', '<SPECIAL_755>', '<SPECIAL_756>', '<SPECIAL_757>', '<SPECIAL_758>', '<SPECIAL_759>', '<SPECIAL_760>', '<SPECIAL_761>', '<SPECIAL_762>', '<SPECIAL_763>', '<SPECIAL_764>', '<SPECIAL_765>', '<SPECIAL_766>', '<SPECIAL_767>', '<SPECIAL_768>', '<SPECIAL_769>', '<SPECIAL_770>', '<SPECIAL_771>', '<SPECIAL_772>', '<SPECIAL_773>', '<SPECIAL_774>', '<SPECIAL_775>', '<SPECIAL_776>', '<SPECIAL_777>', '<SPECIAL_778>', '<SPECIAL_779>', '<SPECIAL_780>', '<SPECIAL_781>', '<SPECIAL_782>', '<SPECIAL_783>', '<SPECIAL_784>', '<SPECIAL_785>', '<SPECIAL_786>', '<SPECIAL_787>', '<SPECIAL_788>', '<SPECIAL_789>', '<SPECIAL_790>', '<SPECIAL_791>', '<SPECIAL_792>', '<SPECIAL_793>', '<SPECIAL_794>', '<SPECIAL_795>', '<SPECIAL_796>', '<SPECIAL_797>', '<SPECIAL_798>', '<SPECIAL_799>', '<SPECIAL_800>', '<SPECIAL_801>', '<SPECIAL_802>', '<SPECIAL_803>', '<SPECIAL_804>', '<SPECIAL_805>', '<SPECIAL_806>', '<SPECIAL_807>', '<SPECIAL_808>', '<SPECIAL_809>', '<SPECIAL_810>', '<SPECIAL_811>', '<SPECIAL_812>', '<SPECIAL_813>', '<SPECIAL_814>', '<SPECIAL_815>', '<SPECIAL_816>', '<SPECIAL_817>', '<SPECIAL_818>', '<SPECIAL_819>', '<SPECIAL_820>', '<SPECIAL_821>', '<SPECIAL_822>', '<SPECIAL_823>', '<SPECIAL_824>', '<SPECIAL_825>', '<SPECIAL_826>', '<SPECIAL_827>', '<SPECIAL_828>', '<SPECIAL_829>', '<SPECIAL_830>', '<SPECIAL_831>', '<SPECIAL_832>', '<SPECIAL_833>', '<SPECIAL_834>', '<SPECIAL_835>', '<SPECIAL_836>', '<SPECIAL_837>', '<SPECIAL_838>', '<SPECIAL_839>', '<SPECIAL_840>', '<SPECIAL_841>', '<SPECIAL_842>', '<SPECIAL_843>', '<SPECIAL_844>', '<SPECIAL_845>', '<SPECIAL_846>', '<SPECIAL_847>', '<SPECIAL_848>', '<SPECIAL_849>', '<SPECIAL_850>', '<SPECIAL_851>', '<SPECIAL_852>', '<SPECIAL_853>', '<SPECIAL_854>', '<SPECIAL_855>', '<SPECIAL_856>', '<SPECIAL_857>', '<SPECIAL_858>', '<SPECIAL_859>', '<SPECIAL_860>', '<SPECIAL_861>', '<SPECIAL_862>', '<SPECIAL_863>', '<SPECIAL_864>', '<SPECIAL_865>', '<SPECIAL_866>', '<SPECIAL_867>', '<SPECIAL_868>', '<SPECIAL_869>', '<SPECIAL_870>', '<SPECIAL_871>', '<SPECIAL_872>', '<SPECIAL_873>', '<SPECIAL_874>', '<SPECIAL_875>', '<SPECIAL_876>', '<SPECIAL_877>', '<SPECIAL_878>', '<SPECIAL_879>', '<SPECIAL_880>', '<SPECIAL_881>', '<SPECIAL_882>', '<SPECIAL_883>', '<SPECIAL_884>', '<SPECIAL_885>', '<SPECIAL_886>', '<SPECIAL_887>', '<SPECIAL_888>', '<SPECIAL_889>', '<SPECIAL_890>', '<SPECIAL_891>', '<SPECIAL_892>', '<SPECIAL_893>', '<SPECIAL_894>', '<SPECIAL_895>', '<SPECIAL_896>', '<SPECIAL_897>', '<SPECIAL_898>', '<SPECIAL_899>', '<SPECIAL_900>', '<SPECIAL_901>', '<SPECIAL_902>', '<SPECIAL_903>', '<SPECIAL_904>', '<SPECIAL_905>', '<SPECIAL_906>', '<SPECIAL_907>', '<SPECIAL_908>', '<SPECIAL_909>', '<SPECIAL_910>', '<SPECIAL_911>', '<SPECIAL_912>', '<SPECIAL_913>', '<SPECIAL_914>', '<SPECIAL_915>', '<SPECIAL_916>', '<SPECIAL_917>', '<SPECIAL_918>', '<SPECIAL_919>', '<SPECIAL_920>', '<SPECIAL_921>', '<SPECIAL_922>', '<SPECIAL_923>', '<SPECIAL_924>', '<SPECIAL_925>', '<SPECIAL_926>', '<SPECIAL_927>', '<SPECIAL_928>', '<SPECIAL_929>', '<SPECIAL_930>', '<SPECIAL_931>', '<SPECIAL_932>', '<SPECIAL_933>', '<SPECIAL_934>', '<SPECIAL_935>', '<SPECIAL_936>', '<SPECIAL_937>', '<SPECIAL_938>', '<SPECIAL_939>', '<SPECIAL_940>', '<SPECIAL_941>', '<SPECIAL_942>', '<SPECIAL_943>', '<SPECIAL_944>', '<SPECIAL_945>', '<SPECIAL_946>', '<SPECIAL_947>', '<SPECIAL_948>', '<SPECIAL_949>', '<SPECIAL_950>', '<SPECIAL_951>', '<SPECIAL_952>', '<SPECIAL_953>', '<SPECIAL_954>', '<SPECIAL_955>', '<SPECIAL_956>', '<SPECIAL_957>', '<SPECIAL_958>', '<SPECIAL_959>', '<SPECIAL_960>', '<SPECIAL_961>', '<SPECIAL_962>', '<SPECIAL_963>', '<SPECIAL_964>', '<SPECIAL_965>', '<SPECIAL_966>', '<SPECIAL_967>', '<SPECIAL_968>', '<SPECIAL_969>', '<SPECIAL_970>', '<SPECIAL_971>', '<SPECIAL_972>', '<SPECIAL_973>', '<SPECIAL_974>', '<SPECIAL_975>', '<SPECIAL_976>', '<SPECIAL_977>', '<SPECIAL_978>', '<SPECIAL_979>', '<SPECIAL_980>', '<SPECIAL_981>', '<SPECIAL_982>', '<SPECIAL_983>', '<SPECIAL_984>', '<SPECIAL_985>', '<SPECIAL_986>', '<SPECIAL_987>', '<SPECIAL_988>', '<SPECIAL_989>', '<SPECIAL_990>', '<SPECIAL_991>', '<SPECIAL_992>', '<SPECIAL_993>', '<SPECIAL_994>', '<SPECIAL_995>', '<SPECIAL_996>', '<SPECIAL_997>', '<SPECIAL_998>', '<SPECIAL_999>']\n",
            "Token '<angry>': 131072\n",
            "Token '<happy>': 131073\n",
            "Token '<neutral>': 131074\n",
            "Token '<sad>': 131075\n",
            "Token '<surprised>': 131076\n",
            "Input IDs: [1, 5969, 15984, 1058, 1032, 131073, 1010, 3210, 1058, 2409, 1395, 1261, 2688, 1046]\n",
            "Decoded: <s>Emotion: <happy>\n",
            "Text: This is a test.\n",
            "Tokenizer files: {}\n",
            "Tokenizer name or path: sarvamai/sarvam-m\n",
            "{'input_ids': [1, 22177, 4304, 1033, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add identifiers for the dataset, ik this is a very bad habbit but just for now :)\n",
        "def idx2emotion(i: int) -> str:\n",
        "    if   i < 95:   return \"angry\"\n",
        "    elif i < 189:  return \"happy\"\n",
        "    elif i < 292:  return \"neutral\"\n",
        "    elif i < 372:  return \"sad\"\n",
        "    else:          return \"surprised\"\n",
        "\n",
        "def add_emotion(example, idx):\n",
        "    example[\"emotion\"] = idx2emotion(idx)\n",
        "    return example\n",
        "\n",
        "raw = load_dataset(\"dvyomkesh/telugu_es_transcription\", split=\"train\")          \\\n",
        "        .map(add_emotion, with_indices=True, num_proc=4)\n",
        "\n",
        "# now split\n",
        "data = raw.train_test_split(test_size=0.1, seed=42)\n",
        "train_ds, val_ds = data[\"train\"], data[\"test\"]\n",
        "\n",
        "# tokenise (no with_indices needed any more)\n",
        "def tokenise(example):\n",
        "    text = f\"Emotion: <{example['emotion']}>\\nText: {example['text']}\"\n",
        "    tok  = tokenizer(text, max_length=256, truncation=True,\n",
        "                     padding=\"max_length\", return_attention_mask=True)\n",
        "    example[\"input_ids\"]      = tok[\"input_ids\"]\n",
        "    example[\"attention_mask\"] = tok[\"attention_mask\"]\n",
        "    example[\"labels\"]         = tok[\"input_ids\"]\n",
        "    return example\n",
        "\n",
        "ds_train = train_ds.map(tokenise, num_proc=4).shuffle(seed=42)\n",
        "ds_val   = val_ds.map(tokenise, num_proc=4).shuffle(seed=42)\n",
        "print(ds_train[0])\n",
        "print(ds_val[5])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204,
          "referenced_widgets": [
            "964d3c761d09402aa3a526c47dcaf6dd",
            "110fe134f23648888a3190f1539a0c71",
            "7a5a6d67bacf4df2a30e40c26133eafc",
            "988b75eecd144e6bad866f4fb027f2c7",
            "493ae5c357af47cfa10e665e9a01e8c1",
            "d365f2b1f816431581b60b7b01573acb",
            "cde3b123b6214487938f85fda5dbaff6",
            "bfaaa007c37c4e968a0c9e4e7b78a83e",
            "5225816cf29948cab34c7a6cef861074",
            "c2852651fb4747da8f217cc519a0d222",
            "d51a8983d6c84175bdd29cdb283502e6",
            "ad08872a225848f4ae86ccd499b42271",
            "e2fcae60810a41428927ba9eeace3515",
            "692154cac4b34abba51a72e16cf402b7",
            "5fd451a0b6d646fea51fbd4027ffbdd3",
            "907befd4d7c64dad80b159a2347c174f",
            "71339c1808bd4995a6ef928500710eef",
            "227985559dab46b9a3fe7b1340b27571",
            "838ae8395a2a4e128917ba7c837ae6ea",
            "d45d00b441104f0a8b196adee8bf52d9",
            "bf5cdb577a264c43a314982a47b755fa",
            "be7957ee0896409e8dba57fe1cec3a4f",
            "1b57a4f4e96a4ffbaa83632da8b0962e",
            "7f212ef89abc42b08fac55badab3219b",
            "b67c4ecb2ed24205ad73a4c4c65be95a",
            "2e6a580fb81648e581e31c20bc7784a2",
            "e609fcdd965d4b5cbafc6bb8d02472c9",
            "0bb5af8a864f4aacbc3b8f4bb5573071",
            "a2252e42e281482ab956f94d10d27094",
            "d4e7d309905543d2b8a726f78b52b279",
            "b2ac83f4e66a46a194f2605f6f8afbc7",
            "f9ae157376db4480ae479f202ce314a7",
            "de64311d03ce4eb1bb7df643d776bbe9"
          ]
        },
        "id": "YTjmPy5Npmj0",
        "outputId": "fad6448a-3e21-4f1f-cf7d-cc4e223c4607"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/456 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "964d3c761d09402aa3a526c47dcaf6dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/410 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad08872a225848f4ae86ccd499b42271"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/46 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b57a4f4e96a4ffbaa83632da8b0962e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'audio': {'path': None, 'array': array([-1.98364258e-04, -7.62939453e-05, -1.83105469e-04, ...,\n",
            "       -3.05175781e-05, -9.15527344e-05,  1.22070312e-04]), 'sampling_rate': 44100}, 'text': 'ఇక్కరు స్టూడింస్ కొట్టుకు చస్తుంటే ఇది కోలేజ్ ప్రాబ్లుమ్ ఎల్లా హోతుంది లోన్ అడా ప్రాబ్లుమ్ హోతుంది', 'emotion': 'neutral', 'input_ids': [1, 5969, 15984, 1058, 1032, 131074, 1010, 3210, 1058, 18364, 28220, 18504, 57726, 26967, 11036, 1184, 2040, 34460, 52024, 41453, 13047, 33031, 2404, 59251, 101612, 48344, 6241, 5027, 21594, 2040, 36552, 1172, 10778, 85194, 2040, 29332, 75714, 2205, 29632, 3776, 7279, 23347, 27880, 16588, 8914, 7577, 2205, 36552, 1172, 10778, 85194, 2040, 29632, 3776, 7279, 23347, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 5969, 15984, 1058, 1032, 131074, 1010, 3210, 1058, 18364, 28220, 18504, 57726, 26967, 11036, 1184, 2040, 34460, 52024, 41453, 13047, 33031, 2404, 59251, 101612, 48344, 6241, 5027, 21594, 2040, 36552, 1172, 10778, 85194, 2040, 29332, 75714, 2205, 29632, 3776, 7279, 23347, 27880, 16588, 8914, 7577, 2205, 36552, 1172, 10778, 85194, 2040, 29632, 3776, 7279, 23347, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n",
            "{'audio': {'path': None, 'array': array([-1.02233887e-03, -6.56127930e-04, -1.02233887e-03, ...,\n",
            "       -3.81469727e-04,  6.10351562e-05,  3.35693359e-04]), 'sampling_rate': 44100}, 'text': 'అది ప్రేకప్ హిప్పిండ్ డాడి', 'emotion': 'sad', 'input_ids': [1, 5969, 15984, 1058, 1032, 131075, 1010, 3210, 1058, 8914, 47309, 12848, 106457, 9506, 2040, 29632, 34360, 22630, 2211, 71392, 34728, 23698, 2211, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [1, 5969, 15984, 1058, 1032, 131075, 1010, 3210, 1058, 8914, 47309, 12848, 106457, 9506, 2040, 29632, 34360, 22630, 2211, 71392, 34728, 23698, 2211, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ds_train[0]['input_ids'])\n",
        "print(ds_val[0]['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YCADjtGCZSr",
        "outputId": "022452b6-2357-4fce-fcc3-f4bbbce792bc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 5969, 15984, 1058, 1032, 131074, 1010, 3210, 1058, 18364, 28220, 18504, 57726, 26967, 11036, 1184, 2040, 34460, 52024, 41453, 13047, 33031, 2404, 59251, 101612, 48344, 6241, 5027, 21594, 2040, 36552, 1172, 10778, 85194, 2040, 29332, 75714, 2205, 29632, 3776, 7279, 23347, 27880, 16588, 8914, 7577, 2205, 36552, 1172, 10778, 85194, 2040, 29632, 3776, 7279, 23347, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
            "[1, 5969, 15984, 1058, 1032, 131073, 1010, 3210, 1058, 8914, 59251, 110192, 58485, 4809, 42003, 22993, 2205, 6681, 2211, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data collator that keeps the 'emotion' field\n",
        "def collate(features):\n",
        "    input_ids      = torch.tensor([f[\"input_ids\"]      for f in features], dtype=torch.long)\n",
        "    attention_mask = torch.tensor([f[\"attention_mask\"] for f in features], dtype=torch.long)\n",
        "    labels         = torch.tensor([f[\"labels\"]         for f in features], dtype=torch.long)\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "    }\n",
        "\n"
      ],
      "metadata": {
        "id": "NKHcnmX2Cnhp"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# callbacks for live loss printing\n",
        "class StepLossPrinter(TrainerCallback):\n",
        "    \"\"\"Print raw loss + EMA every optimisation step.\"\"\"\n",
        "    def __init__(self, beta=0.98):\n",
        "        self.beta, self.ema = beta, None\n",
        "    def on_log(self, args, state, control, logs=None, **kw):\n",
        "        if logs and \"loss\" in logs:\n",
        "            loss = logs[\"loss\"]\n",
        "            self.ema = loss if self.ema is None else self.beta*self.ema + (1-self.beta)*loss\n",
        "            print(f\"step {state.global_step:6} │ loss {loss:6.4f} │ smoothed {self.ema:6.4f}\")\n",
        "\n",
        "class EmotionLossTracker(TrainerCallback):\n",
        "    \"\"\"Bucket losses by emotion for a quick post-mortem.\"\"\"\n",
        "    def __init__(self, emotions=emotion_tokens):\n",
        "        self.buckets = {e.strip(\"<>\"): [] for e in emotions}\n",
        "    def on_train_batch_end(self, args, state, control, **kw):\n",
        "        emolist = kw[\"inputs\"][\"emotion\"]\n",
        "        loss    = kw[\"outputs\"].loss.item()\n",
        "        for e in emolist:                       # same loss for every sample in batch\n",
        "            self.buckets[e].append(loss)\n",
        "    def on_train_end(self, args, state, control, **kw):\n",
        "        print(\"\\n=== average loss by emotion ===\")\n",
        "        for e, v in self.buckets.items():\n",
        "            if v:\n",
        "                print(f\"{e:10}: {sum(v)/len(v):6.4f}\")\n"
      ],
      "metadata": {
        "id": "cxnbpQ5pCpM7"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir = \"/content/drive/MyDrive/sarvam-m-emo-qlora\"\n",
        "ARGS = TrainingArguments(\n",
        "    output_dir        = checkpoint_dir,\n",
        "    per_device_train_batch_size = 8,\n",
        "    gradient_accumulation_steps = 4,       # effective 32\n",
        "    num_train_epochs  = 5,\n",
        "    learning_rate     = 2e-4,\n",
        "    lr_scheduler_type = \"cosine\",\n",
        "    warmup_ratio      = 0.03,\n",
        "    bf16              = True,\n",
        "    logging_steps     = 1,                 # so StepLossPrinter fires each step\n",
        "    save_steps        = 500,\n",
        "    save_total_limit  = 3,\n",
        "    report_to         = \"none\",\n",
        "    gradient_checkpointing = True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "khUiz0hGCrZp"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inspect a batch from your collator\n",
        "sample_batch = collate([ds_train[0]])\n",
        "print(\"Batch keys:\", sample_batch.keys())\n",
        "print(\"input_ids dtype:\", sample_batch['input_ids'].dtype)\n",
        "print(\"labels dtype:\", sample_batch['labels'].dtype)\n",
        "print(\"Shapes:\", sample_batch['input_ids'].shape, sample_batch['labels'].shape)\n",
        "print(\"First few input_ids:\", sample_batch['input_ids'][0][:10])\n",
        "print(\"First few labels:\", sample_batch['labels'][0][:10])\n",
        "print(\"labels present in ds[0]:\", \"labels\" in ds_train[0], \"Shape:\", len(ds_train[0]['labels']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cEe4EoTCtkm",
        "outputId": "39dd8ec9-c302-40d6-fd80-d57a19430ddf"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch keys: dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
            "input_ids dtype: torch.int64\n",
            "labels dtype: torch.int64\n",
            "Shapes: torch.Size([1, 256]) torch.Size([1, 256])\n",
            "First few input_ids: tensor([     1,   5969,  15984,   1058,   1032, 131074,   1010,   3210,   1058,\n",
            "         18364])\n",
            "First few labels: tensor([     1,   5969,  15984,   1058,   1032, 131074,   1010,   3210,   1058,\n",
            "         18364])\n",
            "labels present in ds[0]: True Shape: 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Sample batch from dataset\n",
        "sample = ds_train[0]\n",
        "# If your collate fn returns a batch, do this:\n",
        "batch = {\n",
        "    \"input_ids\": torch.tensor([sample[\"input_ids\"]], dtype=torch.long),\n",
        "    \"attention_mask\": torch.tensor([sample[\"attention_mask\"]], dtype=torch.long),\n",
        "    \"labels\": torch.tensor([sample[\"labels\"]], dtype=torch.long),\n",
        "}\n",
        "# Move batch to the model's device\n",
        "device = next(model.parameters()).device\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "model.train()\n",
        "outputs = model(\n",
        "    input_ids=batch[\"input_ids\"],\n",
        "    attention_mask=batch[\"attention_mask\"],\n",
        "    labels=batch[\"labels\"]\n",
        ")\n",
        "loss = outputs.loss\n",
        "print(\"Loss:\", loss.item())\n",
        "loss.backward()  # <--- Should NOT throw!\n",
        "print(\"Backward pass succeeded!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yI4w2peTCyZl",
        "outputId": "2008509a-1a78-4928-f253-497a280986d9"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.5140984058380127\n",
            "Backward pass succeeded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "last_chkpt = None\n",
        "if os.path.isdir(checkpoint_dir):\n",
        "    ckpts = [d for d in os.listdir(checkpoint_dir) if d.startswith(\"checkpoint-\")]\n",
        "    if ckpts:\n",
        "        last_chkpt = os.path.join(checkpoint_dir, sorted(ckpts, key=lambda x:int(x.split('-')[-1]))[-1])\n",
        "        print(f\"Resuming from {last_chkpt}\")\n",
        "\n",
        "\n",
        "def eval(eval_pred):\n",
        "    \"\"\"\n",
        "    eval_pred is a transformers.EvalPrediction\n",
        "    eval_pred.predictions → logits (np.ndarray)\n",
        "    eval_pred.label_ids   → labels  (np.ndarray)\n",
        "    Return a dict of metric_name: value pairs.\n",
        "    \"\"\"\n",
        "    # example: simple accuracy\n",
        "    import numpy as np\n",
        "    preds = np.argmax(eval_pred.predictions, axis=-1)\n",
        "    labels = eval_pred.label_ids\n",
        "    acc = (preds == labels).mean()\n",
        "    return {\"accuracy\": acc}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model           = model,\n",
        "    args            = ARGS,\n",
        "    train_dataset   = ds_train,\n",
        "    eval_dataset     = ds_val,\n",
        "    data_collator   = collate,\n",
        "    tokenizer       = tokenizer,\n",
        "    callbacks       = [StepLossPrinter(), EmotionLossTracker()],\n",
        "    compute_metrics=eval,\n",
        ")\n",
        "\n",
        "trainer.train(resume_from_checkpoint=last_chkpt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PNpB8OTYDfmv",
        "outputId": "682e42ec-cc5f-4880-9275-95836859be48"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-59-ff7513ff6b25>:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65' max='65' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [65/65 10:55, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.452800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.405800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.409100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.426900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.584300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.415100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.364900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.392100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.443700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.311100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.498800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.428400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.411900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.440600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.309700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.349700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.346500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.472500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.345300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.391100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.302700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.315400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.418000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.392000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.364800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.335900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.293500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.323900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.286000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.349200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.332500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.311400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.393100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.331100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.274800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.281200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.313900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.286800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.219400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.275100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.254600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.227000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.259500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.295800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.251800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.279400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.282700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.251000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.251200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.271600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.197600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.209000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.228700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.225200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.238200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.259700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.225100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.276400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.218600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.222300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.223700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.229700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.202300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step      1 │ loss 0.4528 │ smoothed 0.4528\n",
            "step      2 │ loss 0.4058 │ smoothed 0.4519\n",
            "step      3 │ loss 0.4091 │ smoothed 0.4510\n",
            "step      4 │ loss 0.4269 │ smoothed 0.4505\n",
            "step      5 │ loss 0.5843 │ smoothed 0.4532\n",
            "step      6 │ loss 0.4151 │ smoothed 0.4524\n",
            "step      7 │ loss 0.3649 │ smoothed 0.4507\n",
            "step      8 │ loss 0.3991 │ smoothed 0.4497\n",
            "step      9 │ loss 0.3921 │ smoothed 0.4485\n",
            "step     10 │ loss 0.4437 │ smoothed 0.4484\n",
            "step     11 │ loss 0.3111 │ smoothed 0.4457\n",
            "step     12 │ loss 0.4988 │ smoothed 0.4467\n",
            "step     13 │ loss 0.4284 │ smoothed 0.4464\n",
            "step     14 │ loss 0.4119 │ smoothed 0.4457\n",
            "step     15 │ loss 0.4406 │ smoothed 0.4456\n",
            "step     16 │ loss 0.3097 │ smoothed 0.4428\n",
            "step     17 │ loss 0.3497 │ smoothed 0.4410\n",
            "step     18 │ loss 0.3465 │ smoothed 0.4391\n",
            "step     19 │ loss 0.4725 │ smoothed 0.4398\n",
            "step     20 │ loss 0.3453 │ smoothed 0.4379\n",
            "step     21 │ loss 0.3911 │ smoothed 0.4369\n",
            "step     22 │ loss 0.3027 │ smoothed 0.4343\n",
            "step     23 │ loss 0.3154 │ smoothed 0.4319\n",
            "step     24 │ loss 0.4180 │ smoothed 0.4316\n",
            "step     25 │ loss 0.3920 │ smoothed 0.4308\n",
            "step     26 │ loss 0.3648 │ smoothed 0.4295\n",
            "step     27 │ loss 0.3359 │ smoothed 0.4276\n",
            "step     28 │ loss 0.2935 │ smoothed 0.4249\n",
            "step     29 │ loss 0.2861 │ smoothed 0.4222\n",
            "step     30 │ loss 0.3239 │ smoothed 0.4202\n",
            "step     31 │ loss 0.2860 │ smoothed 0.4175\n",
            "step     32 │ loss 0.3492 │ smoothed 0.4161\n",
            "step     33 │ loss 0.3325 │ smoothed 0.4145\n",
            "step     34 │ loss 0.3114 │ smoothed 0.4124\n",
            "step     35 │ loss 0.3931 │ smoothed 0.4120\n",
            "step     36 │ loss 0.3311 │ smoothed 0.4104\n",
            "step     37 │ loss 0.2748 │ smoothed 0.4077\n",
            "step     38 │ loss 0.2812 │ smoothed 0.4052\n",
            "step     39 │ loss 0.3139 │ smoothed 0.4033\n",
            "step     40 │ loss 0.2868 │ smoothed 0.4010\n",
            "step     41 │ loss 0.2194 │ smoothed 0.3974\n",
            "step     42 │ loss 0.2751 │ smoothed 0.3949\n",
            "step     43 │ loss 0.2546 │ smoothed 0.3921\n",
            "step     44 │ loss 0.2270 │ smoothed 0.3888\n",
            "step     45 │ loss 0.2595 │ smoothed 0.3862\n",
            "step     46 │ loss 0.2958 │ smoothed 0.3844\n",
            "step     47 │ loss 0.2518 │ smoothed 0.3818\n",
            "step     48 │ loss 0.2794 │ smoothed 0.3797\n",
            "step     49 │ loss 0.2827 │ smoothed 0.3778\n",
            "step     50 │ loss 0.2510 │ smoothed 0.3752\n",
            "step     51 │ loss 0.2512 │ smoothed 0.3728\n",
            "step     52 │ loss 0.2716 │ smoothed 0.3707\n",
            "step     53 │ loss 0.1976 │ smoothed 0.3673\n",
            "step     54 │ loss 0.2090 │ smoothed 0.3641\n",
            "step     55 │ loss 0.2287 │ smoothed 0.3614\n",
            "step     56 │ loss 0.2252 │ smoothed 0.3587\n",
            "step     57 │ loss 0.2382 │ smoothed 0.3563\n",
            "step     58 │ loss 0.2597 │ smoothed 0.3543\n",
            "step     59 │ loss 0.2251 │ smoothed 0.3518\n",
            "step     60 │ loss 0.2764 │ smoothed 0.3502\n",
            "step     61 │ loss 0.2186 │ smoothed 0.3476\n",
            "step     62 │ loss 0.2223 │ smoothed 0.3451\n",
            "step     63 │ loss 0.2237 │ smoothed 0.3427\n",
            "step     64 │ loss 0.2297 │ smoothed 0.3404\n",
            "step     65 │ loss 0.2023 │ smoothed 0.3377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:250: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== average loss by emotion ===\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=65, training_loss=0.32102955648532283, metrics={'train_runtime': 665.3174, 'train_samples_per_second': 3.081, 'train_steps_per_second': 0.098, 'total_flos': 7.2173647822848e+16, 'train_loss': 0.32102955648532283, 'epoch': 5.0})"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = trainer.evaluate()   # runs on ds_val once, prints & returns dict\n",
        "print(metrics)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "ptdfaNMaPqRJ",
        "outputId": "3f5c4a5d-cce0-40aa-885c-3b3d34afc6a5"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6/6 00:04]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'eval_loss': 0.39822474122047424, 'eval_accuracy': 0.875594429347826, 'eval_runtime': 10.0426, 'eval_samples_per_second': 4.58, 'eval_steps_per_second': 0.597, 'epoch': 5.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAuBRc8STWx-",
        "outputId": "331d6ce1-9cb5-47ab-cafa-c4f2c19750b8"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.31.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.1 (from gradio)\n",
            "  Downloading gradio_client-1.10.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.31.0-py3-none-any.whl (54.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.1-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.1/323.1 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.31.0 gradio-client-1.10.1 groovy-0.1.2 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.11 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch, transformers, peft, bitsandbytes\n",
        "from peft import PeftModel\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "base_id    = \"sarvamai/sarvam-m\"\n",
        "adapter_id = \"sarvamai/sarvam-m-emo-qlora\"\n",
        "\n",
        "# 4-bit load config (same as training)\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit       = True,\n",
        "    bnb_4bit_quant_type= \"nf4\",\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16,\n",
        "    bnb_4bit_use_double_quant = True,\n",
        ")\n",
        "tok = AutoTokenizer.from_pretrained(adapter_id, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_id,\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "model = PeftModel.from_pretrained(model, adapter_id)  # attach QLoRA\n",
        "model.eval()\n",
        "\n",
        "def chat(inp, temperature, top_p):\n",
        "    prompt = inp if inp.strip().startswith(\"Emotion:\") else f\"Emotion: <neutral>\\nText: {inp}\"\n",
        "    ids = tok(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        out = model.generate(\n",
        "            **ids,\n",
        "            max_new_tokens=64,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            repetition_penalty=1.1,\n",
        "        )\n",
        "    reply = tok.decode(out[0][ids.input_ids.size(1):], skip_special_tokens=True)\n",
        "    return reply.strip()\n",
        "\n",
        "demo = gr.Interface(\n",
        "    fn=chat,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=4, label=\"Input (prepend Emotion: <angry>|<happy>|<sad>|<surprised> if desired)\"),\n",
        "        gr.Slider(0.1, 1.0, 0.7, step=0.05, label=\"Temperature\"),\n",
        "        gr.Slider(0.5, 1.0, 0.9, step=0.05, label=\"Top-p\"),\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Sarvam-M Emotional QLoRA (Telugu)\",\n",
        "    description=\"Small demo – generation is 4-bit on a single GPU/CPU\",\n",
        ")\n",
        "\n",
        "demo.launch(share=True)   # share=True gives a public link\n"
      ],
      "metadata": {
        "id": "zNixsSQITT5J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}